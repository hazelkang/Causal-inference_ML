{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CLAN_Finedust_threshold50_ satisfaction1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOci1mqd2mw4H72A5kZg/kI"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"227ksGb1mu8C","executionInfo":{"status":"ok","timestamp":1601341410731,"user_tz":-540,"elapsed":14513,"user":{"displayName":"Hazel Kang","photoUrl":"","userId":"09199737328986746816"}},"outputId":"ff22671c-1824-4b52-f818-cd8ab2bbcac4","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["pip install qeds fiona geopandas xgboost gensim folium pyLDAvis descartes statspy "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting qeds\n","  Downloading https://files.pythonhosted.org/packages/41/50/509c79a019156862898acb9e23b28e872818c20492da187090167f6702ad/qeds-0.6.2.tar.gz\n","Collecting fiona\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/8b/e8b2c11bed5373c8e98edb85ce891b09aa1f4210fd451d0fb3696b7695a2/Fiona-1.8.17-cp36-cp36m-manylinux1_x86_64.whl (14.8MB)\n","\u001b[K     |████████████████████████████████| 14.8MB 241kB/s \n","\u001b[?25hCollecting geopandas\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a4/e66aafbefcbb717813bf3a355c8c4fc3ed04ea1dd7feb2920f2f4f868921/geopandas-0.8.1-py2.py3-none-any.whl (962kB)\n","\u001b[K     |████████████████████████████████| 972kB 44.8MB/s \n","\u001b[?25hRequirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n","Requirement already satisfied: folium in /usr/local/lib/python3.6/dist-packages (0.8.3)\n","Collecting pyLDAvis\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n","\u001b[K     |████████████████████████████████| 1.6MB 42.0MB/s \n","\u001b[?25hRequirement already satisfied: descartes in /usr/local/lib/python3.6/dist-packages (1.1.0)\n","Collecting statspy\n","  Downloading https://files.pythonhosted.org/packages/52/60/868211536210477e53a2453f16a6559148b4a4430aad8bba6c563b7089b5/statspy-0.1.0a1.tar.gz\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from qeds) (1.0.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from qeds) (2.23.0)\n","Collecting quandl\n","  Downloading https://files.pythonhosted.org/packages/1b/29/185269dbd2e2698c8098b35c52ce73a2c52cf76163e709f9f7789d03ebbb/Quandl-3.5.2-py2.py3-none-any.whl\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from qeds) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from qeds) (1.18.5)\n","Collecting quantecon\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/e3/4fd5f948de917036b5696347b28fa25da7bd7df995e4f9f42db1c3070eb8/quantecon-0.4.8-py3-none-any.whl (230kB)\n","\u001b[K     |████████████████████████████████| 235kB 47.1MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from qeds) (3.2.2)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (from qeds) (0.14.1)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.6/dist-packages (from qeds) (2.5.9)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from qeds) (4.4.1)\n","Requirement already satisfied: pandas_datareader in /usr/local/lib/python3.6/dist-packages (from qeds) (0.8.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from qeds) (0.22.2.post1)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from qeds) (0.10.1)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from qeds) (0.10.2)\n","Collecting click-plugins>=1.0\n","  Downloading https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl\n","Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.6/dist-packages (from fiona) (1.15.0)\n","Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.6/dist-packages (from fiona) (20.2.0)\n","Collecting munch\n","  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n","Collecting cligj>=0.5\n","  Downloading https://files.pythonhosted.org/packages/e4/be/30a58b4b0733850280d01f8bd132591b4668ed5c7046761098d665ac2174/cligj-0.5.0-py3-none-any.whl\n","Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from fiona) (7.1.2)\n","Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.7.1)\n","Collecting pyproj>=2.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c3/071e080230ac4b6c64f1a2e2f9161c9737a2bc7b683d2c90b024825000c0/pyproj-2.6.1.post1-cp36-cp36m-manylinux2010_x86_64.whl (10.9MB)\n","\u001b[K     |████████████████████████████████| 10.9MB 5.2MB/s \n","\u001b[?25hRequirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.1)\n","Requirement already satisfied: branca>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from folium) (0.4.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from folium) (2.11.2)\n","Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.35.1)\n","Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n","Collecting funcy\n","  Downloading https://files.pythonhosted.org/packages/66/89/479de0afbbfb98d1c4b887936808764627300208bb771fcd823403645a36/funcy-1.15-py2.py3-none-any.whl\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->qeds) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->qeds) (2018.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->qeds) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->qeds) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->qeds) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->qeds) (2020.6.20)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from quandl->qeds) (8.5.0)\n","Collecting inflection>=0.3.1\n","  Downloading https://files.pythonhosted.org/packages/59/91/aa6bde563e0085a02a435aa99b49ef75b0a4b062635e606dab23ce18d720/inflection-0.5.1-py2.py3-none-any.whl\n","Requirement already satisfied: numba>=0.38 in /usr/local/lib/python3.6/dist-packages (from quantecon->qeds) (0.48.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.6/dist-packages (from quantecon->qeds) (1.1.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->qeds) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->qeds) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->qeds) (0.10.0)\n","Requirement already satisfied: jdcal in /usr/local/lib/python3.6/dist-packages (from openpyxl->qeds) (1.4.1)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.6/dist-packages (from openpyxl->qeds) (1.0.1)\n","Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->qeds) (1.3.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pandas_datareader->qeds) (4.2.6)\n","Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->qeds) (0.5.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.14.63)\n","Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->folium) (1.1.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (50.3.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.9.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n","Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38->quantecon->qeds) (0.31.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.6/dist-packages (from sympy->quantecon->qeds) (1.1.0)\n","Requirement already satisfied: botocore<1.18.0,>=1.17.63 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.17.63)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n","Building wheels for collected packages: qeds, pyLDAvis, statspy\n","  Building wheel for qeds (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for qeds: filename=qeds-0.6.2-cp36-none-any.whl size=27821 sha256=8626d1cd26972cdc7ae251e3072121f9b6ca7de0fcc73066ca3e9042698cf1e5\n","  Stored in directory: /root/.cache/pip/wheels/b7/0b/74/c09109813c2b6116a2d4f2833c354b24163672f846a50fc7b4\n","  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=dbabb3ef68fc5d07761330a10b55e36da71fba9fdb86beb3335756a8038444aa\n","  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n","  Building wheel for statspy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for statspy: filename=statspy-0.1.0a1-cp36-none-any.whl size=26104 sha256=13b4a195a346bd99e03d59e7e8609d886ac81e21da3c3e3aa4a9234d91157104\n","  Stored in directory: /root/.cache/pip/wheels/f0/5b/95/815b4dc0ba7dd6429eea82364c815d5900cc5758774aaeeb7e\n","Successfully built qeds pyLDAvis statspy\n","Installing collected packages: inflection, quandl, quantecon, qeds, click-plugins, munch, cligj, fiona, pyproj, geopandas, funcy, pyLDAvis, statspy\n","Successfully installed click-plugins-1.1.1 cligj-0.5.0 fiona-1.8.17 funcy-1.15 geopandas-0.8.1 inflection-0.5.1 munch-2.5.0 pyLDAvis-2.1.2 pyproj-2.6.1.post1 qeds-0.6.2 quandl-3.5.2 quantecon-0.4.8 statspy-0.1.0a1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AX61rOSdm7Ll","executionInfo":{"status":"ok","timestamp":1601341412546,"user_tz":-540,"elapsed":16318,"user":{"displayName":"Hazel Kang","photoUrl":"","userId":"09199737328986746816"}},"outputId":"fed7006c-7f35-4eb8-b3a3-6199b5078ca1","colab":{"base_uri":"https://localhost:8080/","height":108}},"source":["#Import\n","import pandas as pd\n","import numpy as np\n","import patsy\n","import statspy as sp\n","from sklearn import linear_model, ensemble, base, neural_network\n","#\n","import statsmodels.formula.api as smf\n","import statsmodels.api as sm\n","from sklearn.utils.testing import ignore_warnings\n","from sklearn.exceptions import ConvergenceWarning\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"aqbs7IQeoMHv","executionInfo":{"status":"ok","timestamp":1601341433414,"user_tz":-540,"elapsed":37173,"user":{"displayName":"Hazel Kang","photoUrl":"","userId":"09199737328986746816"}},"outputId":"b72a6859-56f1-4fb0-915f-4865be115796","colab":{"base_uri":"https://localhost:8080/","height":349}},"source":["#Load file - Demeaned by Location\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","df_path=\"/content/drive/My Drive/ML/KCYPS2010-m1e4e1_new.dta\"\n","df=pd.read_stata(df_path)\n","df.describe()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>year</th>\n","      <th>panel</th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>school_loc</th>\n","      <th>ind_loc</th>\n","      <th>school</th>\n","      <th>grade</th>\n","      <th>coeduw</th>\n","      <th>f_edu</th>\n","      <th>m_edu</th>\n","      <th>health</th>\n","      <th>exercise_time</th>\n","      <th>ego_resilience1</th>\n","      <th>ego_resilience2</th>\n","      <th>ego_resilience3</th>\n","      <th>ego_resilience4</th>\n","      <th>ego_resilience5</th>\n","      <th>ego_resilience6</th>\n","      <th>ego_resilience7</th>\n","      <th>ego_resilience8</th>\n","      <th>ego_resilience9</th>\n","      <th>ego_resilience10</th>\n","      <th>ego_resilience11</th>\n","      <th>ego_resilience12</th>\n","      <th>ego_resilience13</th>\n","      <th>ego_resilience14</th>\n","      <th>satisfaction1</th>\n","      <th>satisfaction2</th>\n","      <th>satisfaction3</th>\n","      <th>homework1_time1</th>\n","      <th>homework1_time2</th>\n","      <th>homework1_time3</th>\n","      <th>homework1_time4</th>\n","      <th>homework2_time1</th>\n","      <th>homework2_time2</th>\n","      <th>homework2_time3</th>\n","      <th>homework2_time4</th>\n","      <th>homework3_time1</th>\n","      <th>...</th>\n","      <th>temp_min_n</th>\n","      <th>temp_min</th>\n","      <th>temp_max_n</th>\n","      <th>temp_max</th>\n","      <th>rainfall_n</th>\n","      <th>rainfall</th>\n","      <th>wind_max_n</th>\n","      <th>wind_max</th>\n","      <th>wind_avg_n</th>\n","      <th>wind_avg</th>\n","      <th>m_study</th>\n","      <th>f_study</th>\n","      <th>female</th>\n","      <th>exercise_time1</th>\n","      <th>loc_yr</th>\n","      <th>hwtime_schday</th>\n","      <th>hwtime_nonschday</th>\n","      <th>withfriend_time_sch</th>\n","      <th>withfriend_time_nsch</th>\n","      <th>satisfaction</th>\n","      <th>aggressive</th>\n","      <th>symptom_phy</th>\n","      <th>depression</th>\n","      <th>ego_resilience</th>\n","      <th>social_cohesion</th>\n","      <th>move</th>\n","      <th>income</th>\n","      <th>aggressive_p1</th>\n","      <th>depression_p1</th>\n","      <th>symptom_phy_p1</th>\n","      <th>ego_resilience_p1</th>\n","      <th>aggressive_p2</th>\n","      <th>depression_p2</th>\n","      <th>symptom_phy_p2</th>\n","      <th>ego_resilience_p2</th>\n","      <th>satisfaction_p3</th>\n","      <th>aggressive_p3</th>\n","      <th>depression_p3</th>\n","      <th>symptom_phy_p3</th>\n","      <th>ego_resilience_p3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25013.000000</td>\n","      <td>25093.000000</td>\n","      <td>19490.000000</td>\n","      <td>25468.000000</td>\n","      <td>25010.000000</td>\n","      <td>25033.000000</td>\n","      <td>13073.000000</td>\n","      <td>24328.000000</td>\n","      <td>24536.000000</td>\n","      <td>25004.000000</td>\n","      <td>25008.000000</td>\n","      <td>13182.000000</td>\n","      <td>13181.000000</td>\n","      <td>13182.000000</td>\n","      <td>13182.000000</td>\n","      <td>13181.000000</td>\n","      <td>13182.000000</td>\n","      <td>13182.000000</td>\n","      <td>13182.000000</td>\n","      <td>13182.000000</td>\n","      <td>13181.000000</td>\n","      <td>13181.000000</td>\n","      <td>13181.000000</td>\n","      <td>13181.000000</td>\n","      <td>13180.000000</td>\n","      <td>23472.000000</td>\n","      <td>23474.000000</td>\n","      <td>23474.000000</td>\n","      <td>24832.000000</td>\n","      <td>24832.000000</td>\n","      <td>24880.000000</td>\n","      <td>24880.000000</td>\n","      <td>24778.000000</td>\n","      <td>24778.000000</td>\n","      <td>24802.000000</td>\n","      <td>24802.000000</td>\n","      <td>24884.000000</td>\n","      <td>...</td>\n","      <td>24891.000000</td>\n","      <td>24891.000000</td>\n","      <td>24891.000000</td>\n","      <td>24891.000000</td>\n","      <td>24891.000000</td>\n","      <td>24891.000000</td>\n","      <td>24891.000000</td>\n","      <td>24891.000000</td>\n","      <td>24891.000000</td>\n","      <td>24891.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25008.000000</td>\n","      <td>25468.000000</td>\n","      <td>24607.000000</td>\n","      <td>24686.000000</td>\n","      <td>24866.000000</td>\n","      <td>24891.000000</td>\n","      <td>23569.000000</td>\n","      <td>16308.000000</td>\n","      <td>16308.000000</td>\n","      <td>16308.000000</td>\n","      <td>13460.000000</td>\n","      <td>24993.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","      <td>25468.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>87913.105623</td>\n","      <td>2012.853760</td>\n","      <td>2.117952</td>\n","      <td>1.467397</td>\n","      <td>13.397601</td>\n","      <td>2741.557414</td>\n","      <td>2746.313570</td>\n","      <td>1.862015</td>\n","      <td>2.972636</td>\n","      <td>2.451771</td>\n","      <td>2.884865</td>\n","      <td>2.698769</td>\n","      <td>3.281715</td>\n","      <td>2.913988</td>\n","      <td>3.074040</td>\n","      <td>3.036871</td>\n","      <td>3.082385</td>\n","      <td>3.043013</td>\n","      <td>2.948866</td>\n","      <td>3.035958</td>\n","      <td>2.682749</td>\n","      <td>2.949325</td>\n","      <td>2.829237</td>\n","      <td>3.037933</td>\n","      <td>2.803657</td>\n","      <td>2.780821</td>\n","      <td>2.958197</td>\n","      <td>3.173976</td>\n","      <td>3.287960</td>\n","      <td>2.791088</td>\n","      <td>3.238434</td>\n","      <td>0.427150</td>\n","      <td>12.297922</td>\n","      <td>0.427653</td>\n","      <td>8.462379</td>\n","      <td>0.483857</td>\n","      <td>8.254702</td>\n","      <td>0.473430</td>\n","      <td>5.805338</td>\n","      <td>0.847452</td>\n","      <td>...</td>\n","      <td>7.308627</td>\n","      <td>3.960305</td>\n","      <td>16.920568</td>\n","      <td>13.723699</td>\n","      <td>3.196835</td>\n","      <td>1.948080</td>\n","      <td>7.116713</td>\n","      <td>7.314519</td>\n","      <td>1.811941</td>\n","      <td>1.880025</td>\n","      <td>13.040051</td>\n","      <td>13.441613</td>\n","      <td>0.459047</td>\n","      <td>1.913988</td>\n","      <td>272.760803</td>\n","      <td>2.211384</td>\n","      <td>2.227036</td>\n","      <td>0.841032</td>\n","      <td>2.089274</td>\n","      <td>0.001044</td>\n","      <td>-0.000357</td>\n","      <td>-0.000072</td>\n","      <td>-0.001104</td>\n","      <td>0.001772</td>\n","      <td>0.000948</td>\n","      <td>0.072444</td>\n","      <td>4.713395</td>\n","      <td>0.000290</td>\n","      <td>-0.000158</td>\n","      <td>0.000303</td>\n","      <td>0.000600</td>\n","      <td>0.000287</td>\n","      <td>-0.000162</td>\n","      <td>0.000298</td>\n","      <td>0.000604</td>\n","      <td>0.000726</td>\n","      <td>0.000284</td>\n","      <td>-0.000162</td>\n","      <td>0.000298</td>\n","      <td>0.000603</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>50006.514034</td>\n","      <td>1.921218</td>\n","      <td>0.757773</td>\n","      <td>0.498946</td>\n","      <td>2.308522</td>\n","      <td>894.825103</td>\n","      <td>895.389902</td>\n","      <td>0.762092</td>\n","      <td>1.736361</td>\n","      <td>0.780727</td>\n","      <td>1.789775</td>\n","      <td>1.580844</td>\n","      <td>0.593584</td>\n","      <td>1.264765</td>\n","      <td>0.650978</td>\n","      <td>0.696299</td>\n","      <td>0.771551</td>\n","      <td>0.659711</td>\n","      <td>0.868586</td>\n","      <td>0.752759</td>\n","      <td>0.917145</td>\n","      <td>0.789322</td>\n","      <td>0.789537</td>\n","      <td>0.768452</td>\n","      <td>0.811332</td>\n","      <td>0.810767</td>\n","      <td>0.776063</td>\n","      <td>0.686779</td>\n","      <td>0.682429</td>\n","      <td>0.883672</td>\n","      <td>0.724015</td>\n","      <td>0.655464</td>\n","      <td>14.657259</td>\n","      <td>0.743685</td>\n","      <td>13.559300</td>\n","      <td>0.749488</td>\n","      <td>13.700982</td>\n","      <td>0.851854</td>\n","      <td>12.110255</td>\n","      <td>1.186044</td>\n","      <td>...</td>\n","      <td>3.089340</td>\n","      <td>3.432631</td>\n","      <td>2.439558</td>\n","      <td>2.750279</td>\n","      <td>4.701747</td>\n","      <td>3.439408</td>\n","      <td>2.055955</td>\n","      <td>2.171004</td>\n","      <td>0.962986</td>\n","      <td>1.027465</td>\n","      <td>3.835263</td>\n","      <td>4.485485</td>\n","      <td>0.498330</td>\n","      <td>1.264866</td>\n","      <td>147.699905</td>\n","      <td>1.630425</td>\n","      <td>2.155110</td>\n","      <td>1.047577</td>\n","      <td>2.007680</td>\n","      <td>0.890658</td>\n","      <td>0.890302</td>\n","      <td>0.914353</td>\n","      <td>0.940720</td>\n","      <td>0.926650</td>\n","      <td>0.890123</td>\n","      <td>0.259226</td>\n","      <td>2.632188</td>\n","      <td>0.047453</td>\n","      <td>0.058434</td>\n","      <td>0.045313</td>\n","      <td>0.070305</td>\n","      <td>0.047391</td>\n","      <td>0.058327</td>\n","      <td>0.045203</td>\n","      <td>0.070110</td>\n","      <td>0.055151</td>\n","      <td>0.047342</td>\n","      <td>0.058376</td>\n","      <td>0.045279</td>\n","      <td>0.070275</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>14201.000000</td>\n","      <td>2010.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>9.000000</td>\n","      <td>-9.000000</td>\n","      <td>1001.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>-9.000000</td>\n","      <td>-9.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>-0.983333</td>\n","      <td>-5.644444</td>\n","      <td>10.175000</td>\n","      <td>5.533333</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>2.775000</td>\n","      <td>2.666667</td>\n","      <td>0.300000</td>\n","      <td>0.266667</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>-3.785720</td>\n","      <td>-1.666522</td>\n","      <td>-1.472274</td>\n","      <td>-1.418390</td>\n","      <td>-4.065019</td>\n","      <td>-3.225905</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>-0.208596</td>\n","      <td>-0.488187</td>\n","      <td>-0.119528</td>\n","      <td>-0.230438</td>\n","      <td>-0.207695</td>\n","      <td>-0.486564</td>\n","      <td>-0.119638</td>\n","      <td>-0.230012</td>\n","      <td>-0.185863</td>\n","      <td>-0.204904</td>\n","      <td>-0.485205</td>\n","      <td>-0.119977</td>\n","      <td>-0.230675</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>45732.000000</td>\n","      <td>2012.000000</td>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","      <td>12.000000</td>\n","      <td>2206.000000</td>\n","      <td>2206.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>2.000000</td>\n","      <td>2.000000</td>\n","      <td>3.000000</td>\n","      <td>2.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>2.000000</td>\n","      <td>3.000000</td>\n","      <td>2.000000</td>\n","      <td>2.000000</td>\n","      <td>2.000000</td>\n","      <td>3.000000</td>\n","      <td>2.000000</td>\n","      <td>2.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>2.000000</td>\n","      <td>3.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>5.125000</td>\n","      <td>1.433333</td>\n","      <td>15.268749</td>\n","      <td>12.033333</td>\n","      <td>0.250000</td>\n","      <td>0.000000</td>\n","      <td>5.650000</td>\n","      <td>5.766666</td>\n","      <td>1.120000</td>\n","      <td>1.116667</td>\n","      <td>12.000000</td>\n","      <td>12.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>158.750000</td>\n","      <td>1.000000</td>\n","      <td>0.500000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>-0.516459</td>\n","      <td>-0.679248</td>\n","      <td>-0.811554</td>\n","      <td>-0.832235</td>\n","      <td>-0.600126</td>\n","      <td>-0.532953</td>\n","      <td>0.000000</td>\n","      <td>3.000000</td>\n","      <td>-0.030510</td>\n","      <td>-0.032831</td>\n","      <td>-0.032709</td>\n","      <td>-0.031780</td>\n","      <td>-0.030423</td>\n","      <td>-0.032738</td>\n","      <td>-0.032433</td>\n","      <td>-0.031676</td>\n","      <td>-0.024723</td>\n","      <td>-0.030508</td>\n","      <td>-0.032835</td>\n","      <td>-0.032488</td>\n","      <td>-0.031734</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>81607.000000</td>\n","      <td>2013.000000</td>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","      <td>13.000000</td>\n","      <td>3023.000000</td>\n","      <td>3023.000000</td>\n","      <td>2.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>2.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>7.144444</td>\n","      <td>3.816667</td>\n","      <td>16.887501</td>\n","      <td>13.650000</td>\n","      <td>1.375000</td>\n","      <td>0.687500</td>\n","      <td>6.775000</td>\n","      <td>6.966667</td>\n","      <td>1.525000</td>\n","      <td>1.566667</td>\n","      <td>12.000000</td>\n","      <td>14.000000</td>\n","      <td>0.000000</td>\n","      <td>2.000000</td>\n","      <td>277.000000</td>\n","      <td>2.000000</td>\n","      <td>2.000000</td>\n","      <td>0.500000</td>\n","      <td>2.000000</td>\n","      <td>-0.041861</td>\n","      <td>0.030267</td>\n","      <td>0.031921</td>\n","      <td>-0.019926</td>\n","      <td>-0.063517</td>\n","      <td>-0.057264</td>\n","      <td>0.000000</td>\n","      <td>4.500000</td>\n","      <td>-0.001802</td>\n","      <td>-0.003242</td>\n","      <td>0.000212</td>\n","      <td>0.000638</td>\n","      <td>-0.001856</td>\n","      <td>-0.003303</td>\n","      <td>-0.000026</td>\n","      <td>0.000811</td>\n","      <td>0.001315</td>\n","      <td>-0.001914</td>\n","      <td>-0.003299</td>\n","      <td>-0.000033</td>\n","      <td>0.000997</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>137606.000000</td>\n","      <td>2015.000000</td>\n","      <td>3.000000</td>\n","      <td>2.000000</td>\n","      <td>15.000000</td>\n","      <td>3511.000000</td>\n","      <td>3511.000000</td>\n","      <td>2.000000</td>\n","      <td>4.000000</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","      <td>1.000000</td>\n","      <td>30.000000</td>\n","      <td>1.000000</td>\n","      <td>20.000000</td>\n","      <td>1.000000</td>\n","      <td>20.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>9.322222</td>\n","      <td>6.566667</td>\n","      <td>18.299999</td>\n","      <td>15.500000</td>\n","      <td>4.187500</td>\n","      <td>2.440000</td>\n","      <td>8.368750</td>\n","      <td>8.766666</td>\n","      <td>2.350000</td>\n","      <td>2.433333</td>\n","      <td>16.000000</td>\n","      <td>16.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>383.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>0.848684</td>\n","      <td>0.589594</td>\n","      <td>0.581082</td>\n","      <td>0.593520</td>\n","      <td>0.598297</td>\n","      <td>0.486844</td>\n","      <td>0.000000</td>\n","      <td>6.000000</td>\n","      <td>0.019340</td>\n","      <td>0.025470</td>\n","      <td>0.016221</td>\n","      <td>0.038664</td>\n","      <td>0.019308</td>\n","      <td>0.025405</td>\n","      <td>0.016153</td>\n","      <td>0.038639</td>\n","      <td>0.032263</td>\n","      <td>0.019253</td>\n","      <td>0.025414</td>\n","      <td>0.016180</td>\n","      <td>0.038778</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>168824.000000</td>\n","      <td>2016.000000</td>\n","      <td>3.000000</td>\n","      <td>2.000000</td>\n","      <td>20.000000</td>\n","      <td>3802.000000</td>\n","      <td>3802.000000</td>\n","      <td>7.000000</td>\n","      <td>8.000000</td>\n","      <td>3.000000</td>\n","      <td>6.000000</td>\n","      <td>6.000000</td>\n","      <td>4.000000</td>\n","      <td>5.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>8.000000</td>\n","      <td>59.000000</td>\n","      <td>14.000000</td>\n","      <td>59.000000</td>\n","      <td>10.000000</td>\n","      <td>55.000000</td>\n","      <td>12.000000</td>\n","      <td>59.000000</td>\n","      <td>9.000000</td>\n","      <td>...</td>\n","      <td>17.666666</td>\n","      <td>13.966666</td>\n","      <td>26.016666</td>\n","      <td>23.200001</td>\n","      <td>36.500000</td>\n","      <td>43.000000</td>\n","      <td>16.700001</td>\n","      <td>16.566666</td>\n","      <td>5.625000</td>\n","      <td>6.466667</td>\n","      <td>23.000000</td>\n","      <td>23.000000</td>\n","      <td>1.000000</td>\n","      <td>4.000000</td>\n","      <td>575.000000</td>\n","      <td>19.000000</td>\n","      <td>28.000000</td>\n","      <td>10.000000</td>\n","      <td>14.000000</td>\n","      <td>1.427503</td>\n","      <td>3.424091</td>\n","      <td>3.513397</td>\n","      <td>4.216280</td>\n","      <td>2.386542</td>\n","      <td>1.961005</td>\n","      <td>1.000000</td>\n","      <td>60.000000</td>\n","      <td>0.175252</td>\n","      <td>0.192094</td>\n","      <td>0.164869</td>\n","      <td>0.735808</td>\n","      <td>0.174833</td>\n","      <td>0.191582</td>\n","      <td>0.165308</td>\n","      <td>0.732145</td>\n","      <td>0.572267</td>\n","      <td>0.174767</td>\n","      <td>0.191935</td>\n","      <td>0.165799</td>\n","      <td>0.731440</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 148 columns</p>\n","</div>"],"text/plain":["                  id          year  ...  symptom_phy_p3  ego_resilience_p3\n","count   25468.000000  25468.000000  ...    25468.000000       25468.000000\n","mean    87913.105623   2012.853760  ...        0.000298           0.000603\n","std     50006.514034      1.921218  ...        0.045279           0.070275\n","min     14201.000000   2010.000000  ...       -0.119977          -0.230675\n","25%     45732.000000   2012.000000  ...       -0.032488          -0.031734\n","50%     81607.000000   2013.000000  ...       -0.000033           0.000997\n","75%    137606.000000   2015.000000  ...        0.016180           0.038778\n","max    168824.000000   2016.000000  ...        0.165799           0.731440\n","\n","[8 rows x 148 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"AkQEsWS006ja","executionInfo":{"status":"ok","timestamp":1601341433416,"user_tz":-540,"elapsed":37167,"user":{"displayName":"Hazel Kang","photoUrl":"","userId":"09199737328986746816"}}},"source":["## (Outcome1) satisfaction1\n","\n","#satisfaction, health, aggressive, depression\n","\n","#Set dataset/cleaning\n","formula=\"\"\"\n","satisfaction1 ~ pm10_bad + income + age + p_study + temp_avg + rainfall + wind_avg\n","\"\"\"\n","\n","df[\"p_study\"] = ((df[\"m_study\"] >=12)|(df[\"f_study\"] >=12)).astype(int)\n","\n","satisfaction1, X = patsy.dmatrices(formula, df, return_type=\"dataframe\")\n","# some categories are empty after dropping rows will Null, drop now\n","#X = X.loc[:, X.sum() > 0]\n","satisfaction1 = satisfaction1.iloc[:, 0]\n","treatment_variable = \"pm10_bad\"\n","treatment = X[\"pm10_bad\"]\n","Xl = X.drop([\"Intercept\",\"pm10_bad\"], axis=1)\n","loc_id = df.loc[X.index, \"ind_loc\"].astype(\"category\")\n","\n","import re\n","# remove [ ] from names for compatibility with xgboost\n","Xl = Xl.rename(columns=lambda x: re.sub('\\[|\\]','_',x))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"euZjVZUpc6mo","executionInfo":{"status":"ok","timestamp":1601341433938,"user_tz":-540,"elapsed":37684,"user":{"displayName":"Hazel Kang","photoUrl":"","userId":"09199737328986746816"}}},"source":["################################################################################\n","\n","##Step 1: Best ML \n","# Def of SE\n","def get_treatment_se(fit, cluster_id, rows=None):\n","    if cluster_id is not None:\n","        if rows is None:\n","            rows = [True] * len(cluster_id)\n","        vcov = sm.stats.sandwich_covariance.cov_cluster(fit, cluster_id.loc[rows])\n","        return np.sqrt(np.diag(vcov))\n","\n","    return fit.HC0_se\n","\n","# Def of Generic ML model\n","def generic_ml_model(x, y, treatment, model, n_split=100, n_group=5, cluster_id=None):\n","    nobs = x.shape[0]\n","\n","    blp = np.zeros((n_split, 2))\n","    blp_se = blp.copy()\n","    blp_ci_l = blp.copy()\n","    blp_ci_u = blp.copy()\n","    blp_pvalue= np.zeros((n_split, 4))\n","\n","    gate = np.zeros((n_split, n_group))\n","    affected = np.zeros((n_split, 3))    \n","    gate_se = affected.copy()\n","    affected_ci_l = affected.copy()\n","    affected_ci_u = affected.copy()\n","    affected_pvalue = np.zeros((n_split, 6))\n","\n","    blp_new = np.zeros((1, 2))\n","    affected_new = np.zeros((1, 3))\n","\n","    baseline = np.zeros((nobs, n_split))\n","    cate = baseline.copy()\n","    lamb = np.zeros((n_split, 2))\n","\n","    for i in range(n_split):\n","        main = np.random.rand(nobs) > 0.5\n","        rows1 = ~main & (treatment == 1)\n","        rows0 = ~main & (treatment == 0)\n","\n","        mod1 = base.clone(model).fit(x.loc[rows1, :], (y.loc[rows1]))\n","        mod0 = base.clone(model).fit(x.loc[rows0, :], (y.loc[rows0]))\n","\n","        B = mod0.predict(x)\n","        S = mod1.predict(x) - B\n","\n","        if B.var()==0:\n","          B = B + np.random.normal(0, 0.1, nobs)\n","        if S.var()==0:\n","          S = S + np.random.normal(0, 0.1, nobs)\n","\n","        baseline[:, i] = B\n","        cate[:, i] = S\n","        ES = S.mean()\n","\n","        #Creating weights\n","        p = treatment.mean()\n","        num=main[main==1].shape[0]\n","\n","        weight=np.repeat((1/(p*(1-p))), num)\n","\n","################################################################################\n","\n","        ## BLP\n","        # assume P(treat|x) = P(treat) = mean(treat)\n","        p = treatment.mean()\n","        reg_df = pd.DataFrame(dict(\n","            y=y, B=B, treatment=treatment, S=S, main=main, excess_S=S-ES\n","        ))\n","        reg =smf.wls(\"y ~ B + S + I(treatment-p) + I((treatment-p)*(S-ES))\", data=reg_df.loc[main, :], weights = weight)\n","        #reg =smf.ols(\"y ~ B + I(treatment-p) + I((treatment-p)*(S-ES))\", data=reg_df.loc[main, :]);\n","        reg_fit = reg.fit(cov_type='cluster', cov_kwds={'groups': cluster_id[main] })\n","\n","        blp_se[i, :] = get_treatment_se(reg_fit, cluster_id, main)[3:]\n","        \n","        #beta1 BLP\n","        beta1_blp = reg_fit.params.iloc[3]\n","        beta1_blp_ci_l = reg_fit.conf_int(alpha=0.05, cols=None).iloc[:,0][3]\n","        beta1_blp_ci_u= reg_fit.conf_int(alpha=0.05, cols=None).iloc[:,1][3]\n","\n","        #beta2 BLP\n","        beta2_blp = reg_fit.params.iloc[4]\n","        beta2_blp_ci_l = reg_fit.conf_int(alpha=0.05, cols=None).iloc[:,0][4]\n","        beta2_blp_ci_u= reg_fit.conf_int(alpha=0.05, cols=None).iloc[:,1][4]\n","\n","        #beta1, beta2\n","        blp[i,:]=beta1_blp, beta2_blp\n","\n","        #BLP pvalue\n","        blp1_pvalue=reg_fit.pvalues[2]\n","        blp2_pvalue=reg_fit.pvalues[3]\n","\n","        blp1_1=(beta1_blp<0)*1*(blp1_pvalue/2) + (beta1_blp>0)*1*(1-blp1_pvalue/2)\n","        blp1_2=(beta1_blp<0)*1*(1-blp1_pvalue/2) + (beta1_blp>0)*1*(blp1_pvalue/2)\n","        #blp1=min(1, 4*min(blp1_1, blp1_2))\n","\n","        blp2_1=(beta2_blp<0)*1*(blp2_pvalue/2) + (beta2_blp>0)*1*(1-blp2_pvalue/2)\n","        blp2_2=(beta2_blp<0)*1*(1-blp2_pvalue/2) + (beta2_blp>0)*1*(blp2_pvalue/2)\n","        #blp2=min(1, 4*min(blp2_1, blp2_2))\n","\n","        blp_pvalue[i,:] = blp1_1, blp1_2, blp2_1, blp2_2\n","\n","        blp_ci_l[i,:]=beta1_blp_ci_l, beta2_blp_ci_l\n","        blp_ci_u[i,:]=beta1_blp_ci_u, beta2_blp_ci_u\n","\n","        lamb[i, 0] = reg_fit.params.iloc[-1]**2 * S.var()\n","\n","################################################################################\n","\n","        ## GATEs\n","        cutoffs = np.quantile(S, np.linspace(0,1, n_group + 1))\n","        cutoffs[-1] += 1\n","        for k in range(n_group):\n","            reg_df[f\"G{k}\"] = (cutoffs[k] <= S) & (S < cutoffs[k+1])\n","\n","        g_form = \"y ~ B + S + \" + \" + \".join([f\"I((treatment-p)*G{k})\" for k in range(n_group)])\n","        g_reg = smf.wls(g_form, data=reg_df.loc[main, :], weights = weight)\n","        #g_reg = smf.ols(g_form, data=reg_df.loc[main, :])\n","        g_fit = g_reg.fit(cov_type='cluster', cov_kwds={'groups': cluster_id[main]})\n","        gate[i, :] = g_fit.params.values[3:] #g_fit.params.filter(regex=\"G\").values\n","\n","        lamb[i, 1] = (gate[i,:]**2).sum()/n_group\n","\n","################################################################################\n","\n","        ## Most/Least affected\n","        g_form = \"y ~ B + S +  \" + \" + \".join([f\"I((treatment-p)*G{k})\" for k in range(n_group)])\n","        g_reg = smf.wls(g_form, data=reg_df.loc[main, :], weights = weight)\n","        g_fit = g_reg.fit(cov_type='cluster', cov_kwds={'groups': cluster_id[main]})\n","\n","\n","\n","        #Most affected \n","        most_coef=g_fit.params[6]        \n","        most_pvalue=g_fit.pvalues[6]\n","        most_ci_l=g_fit.conf_int(alpha=0.05, cols=None).iloc[6,0]\n","        most_ci_u=g_fit.conf_int(alpha=0.05, cols=None).iloc[6,1]\n","\n","\n","        #Least affected \n","        least_coef=g_fit.params[2]\n","        least_pvalue=g_fit.pvalues[2]\n","        least_ci_l=g_fit.conf_int(alpha=0.05, cols=None).iloc[2,0]\n","        least_ci_u=g_fit.conf_int(alpha=0.05, cols=None).iloc[2,1]\n","\n","        #Difference\n","        test=g_fit.t_test('I((treatment - p) * G4)-I((treatment - p) * G0)=0')\n","        dir(test)\n","        diff_coef= test.effect\n","        #diff_coef=most_coef-least_coef\n","        diff_pvalue=test.pvalue\n","        diff_ci_l=test.conf_int(alpha=0.05)[0][0]\n","        diff_ci_u=test.conf_int(alpha=0.05)[0][1]\n","\n","        #pvalue\n","        most_p1=(most_coef<0)*1*(most_pvalue/2)+(most_coef>0)*1*(1-most_pvalue/2)\n","        most_p2=(most_coef<0)*1*(1-most_pvalue/2)+(most_coef>0)*1*(most_pvalue/2)\n","        least_p1=(least_coef<0)*1*(least_pvalue/2)+(least_coef>0)*1*(1-least_pvalue/2)\n","        least_p2=(least_coef<0)*1*(1-least_pvalue/2)+(least_coef>0)*1*(least_pvalue/2)\n","        diff_p1=(diff_coef<0)*1*(diff_pvalue/2)+(diff_coef>0)*1*(1-diff_pvalue/2)\n","        diff_p2=(diff_coef<0)*1*(1-diff_pvalue/2)+(diff_coef>0)*1*(diff_pvalue/2)\n","\n","        affected[i,:]=most_coef,least_coef,diff_coef\n","        affected_ci_l[i,:]=most_ci_l,least_ci_l,diff_ci_l\n","        affected_ci_u[i,:]=most_ci_u,least_ci_u,diff_ci_u\n","        affected_pvalue[i,:]=most_p1, most_p2, least_p1, least_p2, diff_p1, diff_p2\n","\n","\n","################################################################################\n","    blp_pvalue= np.nanmedian(blp_pvalue, axis=0)\n","    affected_pvalue= np.nanmedian(affected_pvalue, axis=0)\n","\n","    blp_new[0][0] = min(1, 4*min(blp_pvalue[0], blp_pvalue[1]))\n","    blp_new[0][1] = min(1, 4*min(blp_pvalue[2], blp_pvalue[3]))\n","    blp_pvalue = blp_new\n","\n","    affected_new[0][0] = min(1, 4*min(affected_pvalue[0], affected_pvalue[1]))\n","    affected_new[0][1] = min(1, 4*min(affected_pvalue[2], affected_pvalue[3]))\n","    affected_new[0][2] = min(1, 4*min(affected_pvalue[4], affected_pvalue[5]))\n","    affected_pvalue = affected_new\n","\n","\n","    out = dict(\n","        blp = blp,\n","        blp_ci_l=blp_ci_l, \n","        blp_ci_u=blp_ci_u,\n","        blp_pvalue=blp_pvalue, \n","        gate=gate,\n","        gate_se=gate_se, \n","        blp_se=blp_se,\n","        affected = affected,\n","        affected_ci_l =affected_ci_l, \n","        affected_ci_u= affected_ci_u, \n","        affected_pvalue = affected_pvalue,\n","        Lambda=lamb, \n","        baseline=baseline, \n","        cate=cate,\n","        name=type(model).__name__\n","    )\n","    return out"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"gADcpC3WMXdn","executionInfo":{"status":"ok","timestamp":1601341433939,"user_tz":-540,"elapsed":37679,"user":{"displayName":"Hazel Kang","photoUrl":"","userId":"09199737328986746816"}}},"source":["#Step 1: Tune ML hyperparameters: Best: Boosting /random forest\n","import xgboost as xgb\n","models = [\n","    xgb.XGBRegressor(n_estimators=100, max_depth=3, reg_lambda=2.0, reg_alpha=0.0, objective=\"reg:squarederror\"),\n","    ensemble.RandomForestRegressor(n_estimators=100, min_samples_leaf=20)\n","]\n","#models = [\n","#    linear_model.ElasticNetCV(cv=5, n_alphas=25, max_iter=1000, tol=1e-3, n_jobs=-1, l1_ratio=0.95),\n","#    ensemble.RandomForestRegressor(n_estimators=100, min_samples_leaf=20),\n","#    xgb.XGBRegressor(n_estimators=100, max_depth=3, reg_lambda=2.0, reg_alpha=0.0, objective=\"reg:squarederror\"),\n","#    neural_network.MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, activation=\"relu\", solver=\"adam\", tol=1e-3, early_stopping=True, alpha=0.0001, learning_rate=\"constant\", shuffle=True)\n","#]\n","\n","#Set parameters\n","x = Xl\n","treatment=treatment\n","n_split=100\n","cluster_id=loc_id\n","\n","#Tune parameters\n","y=satisfaction1\n","n_group=5"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0jna71Eb9Ok","executionInfo":{"status":"ok","timestamp":1601341433940,"user_tz":-540,"elapsed":37675,"user":{"displayName":"Hazel Kang","photoUrl":"","userId":"09199737328986746816"}}},"source":["\n","kw = dict(x=x, treatment= treatment, n_split=n_split, n_group=n_group, cluster_id=cluster_id)\n","@ignore_warnings(category=ConvergenceWarning)\n","\n","\n","def evaluate_models(models, y, **other_kw):\n","    all_kw = kw.copy()\n","    all_kw[\"y\"] = y\n","    all_kw.update(other_kw)\n","    return list(map(lambda x: generic_ml_model(model=x, **all_kw), models))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xUFHSDycBcX","executionInfo":{"status":"ok","timestamp":1601341725564,"user_tz":-540,"elapsed":329293,"user":{"displayName":"Hazel Kang","photoUrl":"","userId":"09199737328986746816"}}},"source":["results = evaluate_models(models=models, y=y)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCOij0ngbwYF","executionInfo":{"status":"ok","timestamp":1601341725572,"user_tz":-540,"elapsed":329296,"user":{"displayName":"Hazel Kang","photoUrl":"","userId":"09199737328986746816"}}},"source":["#Create SES variable\n","#df[\"p_study\"] = ((df[\"m_study\"] >=12)|(df[\"f_study\"] >=12)).astype(int)\n","\n","#Set variables: 2\n","df2 = df.loc[X.index, :]\n","variables = [\n","    \"income\", \"p_study\",\"age\"\n","    ]"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sj2-emo5T_ur","executionInfo":{"status":"ok","timestamp":1601341725576,"user_tz":-540,"elapsed":329293,"user":{"displayName":"Hazel Kang","photoUrl":"","userId":"09199737328986746816"}}},"source":["###THIS\n","## CLAN: Most & Least Affected ********************\n","\n","#Create SES variable\n","#df[\"p_study\"] = ((df[\"m_study\"] >=12)|(df[\"f_study\"] >=12)).astype(int)\n","\n","def cov_mean_by_group(y, res, cluster_id):\n","    #n_group = 3\n","    nob=res[\"gate\"].shape[0]\n","    gate = np.zeros((nob,3))\n","    gate_se = gate.copy()\n","    gate_ci_l=gate.copy()\n","    gate_ci_u=gate.copy()\n","    affected_pvalue = np.zeros((nob,6))\n","    affected_new=np.zeros((1,3))\n","    dat = y.to_frame()\n","    thres=0.2\n","\n","    for i in range(res[\"cate\"].shape[1]):\n","        S = res[\"cate\"][:, i]\n","        high_effect=np.quantile(S, 1-thres)\n","        low_effect=np.quantile(S, thres)\n","        h = (S > high_effect)*1\n","        l = (S < low_effect)*1\n","\n","        if h.var()==0:\n","          h=(np.random.uniform(len(h))<0.1)*1\n","        if l.var()==0:\n","          l=(np.random.uniform(len(l))<0.1)*1\n","\n","        dat[\"h\"] =h\n","        dat[\"l\"] =l\n","\n","        main= (dat[\"h\"]==1) | (dat[\"l\"]==1)\n","\n","\n","        ## Most/Least affected\n","        reg_df = pd.DataFrame(dict(\n","            y=y, h=h, l=l, main=main\n","        ))\n","        g_form = \"y ~ -1 + h + l\"\n","        g_reg = smf.ols(g_form, data=reg_df.loc[main,:])\n","        g_fit = g_reg.fit(cov_type='cluster', cov_kwds={'groups': cluster_id[main]})\n","\n","        #Most affected \n","        most_coef=g_fit.params[0]        \n","        most_pvalue=g_fit.pvalues[0]\n","        most_ci_l=g_fit.conf_int(alpha=0.05, cols=None).iloc[0,0]\n","        most_ci_u=g_fit.conf_int(alpha=0.05, cols=None).iloc[0,1]\n","\n","\n","        #Least affected \n","        least_coef=g_fit.params[1]\n","        least_pvalue=g_fit.pvalues[1]\n","        least_ci_l=g_fit.conf_int(alpha=0.05, cols=None).iloc[1,0]\n","        least_ci_u=g_fit.conf_int(alpha=0.05, cols=None).iloc[1,1]\n","\n","        #Difference\n","        test=g_fit.t_test('h-l=0')\n","        dir(test)\n","        diff_coef=most_coef-least_coef\n","        diff_pvalue=test.pvalue\n","        diff_ci_l=test.conf_int(alpha=0.05)[0][0]\n","        diff_ci_u=test.conf_int(alpha=0.05)[0][1]\n","\n","        #pvalue\n","        most_p1=(most_coef<0)*1*(most_pvalue/2)+(most_coef>0)*1*(1-most_pvalue/2)\n","        most_p2=(most_coef<0)*1*(1-most_pvalue/2)+(most_coef>0)*1*(most_pvalue/2)\n","        least_p1=(least_coef<0)*1*(least_pvalue/2)+(least_coef>0)*1*(1-least_pvalue/2)\n","        least_p2=(least_coef<0)*1*(1-least_pvalue/2)+(least_coef>0)*1*(least_pvalue/2)\n","        diff_p1=(diff_coef<0)*1*(diff_pvalue/2)+(diff_coef>0)*1*(1-diff_pvalue/2)\n","        diff_p2=(diff_coef<0)*1*(1-diff_pvalue/2)+(diff_coef>0)*1*(diff_pvalue/2)\n","\n","        gate[i,:]=most_coef,least_coef,diff_coef\n","        gate_ci_l[i,:]=most_ci_l, least_ci_l, diff_ci_l\n","        gate_ci_u[i,:]=most_ci_u, least_ci_u, diff_ci_u\n","        affected_pvalue[i,:]=most_p1, most_p2, least_p1, least_p2, diff_p1, diff_p2\n","#####\n","    affected_pvalue=np.nanmedian(affected_pvalue, axis=0)\n","\n","    affected_new[0][0] = min(1, 4*min(affected_pvalue[0], affected_pvalue[1]))\n","    affected_new[0][1] = min(1, 4*min(affected_pvalue[2], affected_pvalue[3]))\n","    affected_new[0][2] = min(1, 4*min(affected_pvalue[4], affected_pvalue[5]))\n","    affected_pvalue = affected_new.ravel()\n","\n","\n","    out = pd.DataFrame(dict(\n","        mean=np.nanmedian(gate, axis=0),\n","        se=np.nanmedian(gate_se, axis=0),\n","        affected_pvalue=affected_pvalue,\n","        ci_l=np.nanmedian(gate_ci_l,axis=0), \n","        ci_u=np.nanmedian(gate_ci_u,axis=0),\n","        group=[\"20% Most\",\"20% Least\", \"Difference\"]\n","    ))\n","\n","    return out\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgrQA4rHUxFK","executionInfo":{"status":"ok","timestamp":1601341729156,"user_tz":-540,"elapsed":332857,"user":{"displayName":"Hazel Kang","photoUrl":"","userId":"09199737328986746816"}},"outputId":"841b9f5f-5141-4c30-eb86-34726b7a00e4","colab":{"base_uri":"https://localhost:8080/","height":135}},"source":["cov_mean_by_group(df2[\"income\"], results[0], cluster_id)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean</th>\n","      <th>se</th>\n","      <th>affected_pvalue</th>\n","      <th>ci_l</th>\n","      <th>ci_u</th>\n","      <th>group</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.436827</td>\n","      <td>0.0</td>\n","      <td>2.055968e-288</td>\n","      <td>4.208796</td>\n","      <td>4.674839</td>\n","      <td>20% Most</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.914090</td>\n","      <td>0.0</td>\n","      <td>1.809532e-291</td>\n","      <td>4.654538</td>\n","      <td>5.167426</td>\n","      <td>20% Least</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.487965</td>\n","      <td>0.0</td>\n","      <td>4.470142e-03</td>\n","      <td>-0.783624</td>\n","      <td>-0.185037</td>\n","      <td>Difference</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       mean   se  affected_pvalue      ci_l      ci_u       group\n","0  4.436827  0.0    2.055968e-288  4.208796  4.674839    20% Most\n","1  4.914090  0.0    1.809532e-291  4.654538  5.167426   20% Least\n","2 -0.487965  0.0     4.470142e-03 -0.783624 -0.185037  Difference"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"5jwYqjnEphNq","executionInfo":{"status":"ok","timestamp":1601341756349,"user_tz":-540,"elapsed":360035,"user":{"displayName":"Hazel Kang","photoUrl":"","userId":"09199737328986746816"}},"outputId":"7e2053cd-4b08-491f-8894-fdd5d76835c6","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def compute_group_means_for_results(results):\n","    to_cat = []\n","    for res in results:\n","        for v in variables:\n","            to_cat.append(\n","                cov_mean_by_group(df2[v], res, loc_id)\n","                .assign(method=res[\"name\"], variable=v)\n","            )\n","\n","    group_means = pd.concat(to_cat, ignore_index=True)\n","    #group_means[\"plus2sd\"] = group_means.eval(\"mean + 1.96*se\")\n","    #group_means[\"minus2sd\"] = group_means.eval(\"mean - 1.96*se\")\n","    return group_means\n","\n","#######\n","group_means = compute_group_means_for_results(results)\n","#######\n","#Rename Method Name\n","nob=group_means.shape[0]\n","for i in range(nob):\n","  if group_means[\"method\"][i]=='ElasticNetCV':\n","    group_means[\"method\"][i]='Elastic Net'\n","  elif group_means[\"method\"][i]=='MLPRegressor':\n","    group_means[\"method\"][i]='Neural Network'\n","  elif group_means[\"method\"][i]=='RandomForestRegressor':\n","    group_means[\"method\"][i]='Random Forest'\n","  else:\n","    group_means[\"method\"][i]='Booster'\n","\n","#Rename variable name\n","for i in range(nob):\n","  if group_means[\"variable\"][i]=='income':\n","    group_means[\"variable\"][i]='Income'\n","  elif group_means[\"variable\"][i]=='age':\n","    group_means[\"variable\"][i]='Age'\n","  else:\n","    group_means[\"variable\"][i]='SES'\n","\n","#Rename columns\n","group_means.rename(columns={'ci_u':'Upper CI','ci_l':'Lower CI', 'affected_pvalue':'pvalue'}, inplace=True)\n","\n","\n","group_means=group_means[[\"mean\", \"pvalue\", \"Lower CI\", \"Upper CI\",\"method\",\"variable\"]]\n","print(group_means)\n","\n","#Save CLAN to csv\n","group_means.to_csv(\"/content/drive/My Drive/ML/new2/Stage3_CLAN_satisfaction1.csv\", index=False, header=True)\n","\n","#for Table\n","clan_path=\"/content/drive/My Drive/ML/new2/Stage3_CLAN_satisfaction1.csv\"\n","clan=pd.read_csv(clan_path)\n","clan=pd.DataFrame(dict(mean=clan[\"mean\"],se=clan[\"pvalue\"],lower=clan[\"Lower CI\"], upper=clan[\"Upper CI\"]))\n","print(clan)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"stream","text":["         mean         pvalue   Lower CI   Upper CI         method variable\n","0    4.436827  2.055968e-288   4.208796   4.674839        Booster   Income\n","1    4.914090  1.809532e-291   4.654538   5.167426        Booster   Income\n","2   -0.487965   4.470142e-03  -0.783624  -0.185037        Booster   Income\n","3    0.950191   0.000000e+00   0.937999   0.962556        Booster      SES\n","4    0.961534   0.000000e+00   0.949834   0.972967        Booster      SES\n","5   -0.010569   4.583956e-01  -0.027811   0.006554        Booster      SES\n","6   14.202703   0.000000e+00  13.870509  14.525090        Booster      Age\n","7   12.491006   0.000000e+00  12.324847  12.663485        Booster      Age\n","8    1.658204   9.010222e-17   1.288454   2.062461        Booster      Age\n","9    4.493447  1.960358e-284   4.243432   4.748002  Random Forest   Income\n","10   4.826090   0.000000e+00   4.606462   5.067269  Random Forest   Income\n","11  -0.337240   7.831358e-02  -0.639690  -0.014337  Random Forest   Income\n","12   0.963165   0.000000e+00   0.952744   0.974057  Random Forest      SES\n","13   0.971774   0.000000e+00   0.963506   0.980104  Random Forest      SES\n","14  -0.008847   2.337038e-01  -0.020011   0.002584  Random Forest      SES\n","15  13.901014   0.000000e+00  13.592132  14.202465  Random Forest      Age\n","16  12.598845   0.000000e+00  12.378269  12.815744  Random Forest      Age\n","17   1.296658   4.715097e-10   0.880412   1.694609  Random Forest      Age\n","         mean             se      lower      upper\n","0    4.436827  2.055968e-288   4.208796   4.674839\n","1    4.914090  1.809532e-291   4.654538   5.167426\n","2   -0.487965   4.470142e-03  -0.783624  -0.185037\n","3    0.950191   0.000000e+00   0.937999   0.962556\n","4    0.961534   0.000000e+00   0.949834   0.972967\n","5   -0.010569   4.583956e-01  -0.027811   0.006554\n","6   14.202703   0.000000e+00  13.870509  14.525090\n","7   12.491006   0.000000e+00  12.324847  12.663485\n","8    1.658204   9.010222e-17   1.288454   2.062461\n","9    4.493447  1.960358e-284   4.243432   4.748002\n","10   4.826090   0.000000e+00   4.606462   5.067269\n","11  -0.337240   7.831358e-02  -0.639690  -0.014337\n","12   0.963165   0.000000e+00   0.952744   0.974057\n","13   0.971774   0.000000e+00   0.963506   0.980104\n","14  -0.008847   2.337038e-01  -0.020011   0.002584\n","15  13.901014   0.000000e+00  13.592132  14.202465\n","16  12.598845   0.000000e+00  12.378269  12.815744\n","17   1.296658   4.715097e-10   0.880412   1.694609\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nP4VomUNTK1H","executionInfo":{"status":"ok","timestamp":1601341756696,"user_tz":-540,"elapsed":360368,"user":{"displayName":"Hazel Kang","photoUrl":"","userId":"09199737328986746816"}},"outputId":"563b4ac6-bb40-40a1-9b00-f9a59fea4373","colab":{"base_uri":"https://localhost:8080/","height":230}},"source":["#Step 3 : Create table\n","clan_path=\"/content/drive/My Drive/ML/new2/Stage3_CLAN_satisfaction1.csv\"\n","clan=pd.read_csv(clan_path)\n","\n","\n","#Create Dataframe\n","c = np.arange(54).reshape(9,6)\n","c_df=pd.DataFrame(c)\n","\n","#1st row: mean\n","#income\n","c_df.iloc[0,0]=round(clan.iloc[0,0],3)\n","c_df.iloc[0,1]=round(clan.iloc[1,0],3)\n","c_df.iloc[0,2]=round(clan.iloc[2,0],3)\n","c_df.iloc[0,3]=round(clan.iloc[9,0],3)\n","c_df.iloc[0,4]=round(clan.iloc[10,0],3)\n","c_df.iloc[0,5]=round(clan.iloc[11,0],3)\n","#SES\n","c_df.iloc[3,0]=round(clan.iloc[3,0],3)\n","c_df.iloc[3,1]=round(clan.iloc[4,0],3)\n","c_df.iloc[3,2]=round(clan.iloc[5,0],3)\n","c_df.iloc[3,3]=round(clan.iloc[12,0],3)\n","c_df.iloc[3,4]=round(clan.iloc[13,0],3)\n","c_df.iloc[3,5]=round(clan.iloc[14,0],3)\n","#age\n","c_df.iloc[6,0]=round(clan.iloc[6,0],3)\n","c_df.iloc[6,1]=round(clan.iloc[7,0],3)\n","c_df.iloc[6,2]=round(clan.iloc[8,0],3)\n","c_df.iloc[6,3]=round(clan.iloc[15,0],3)\n","c_df.iloc[6,4]=round(clan.iloc[16,0],3)\n","c_df.iloc[6,5]=round(clan.iloc[17,0],3)\n","\n","#2nd row: CI\n","#income\n","c_df.iloc[1,0]=f\"({round(clan.iloc[0,2],3)},{round(clan.iloc[0,3],3)})\"\n","c_df.iloc[1,1]=f\"({round(clan.iloc[1,2],3)},{round(clan.iloc[1,3],3)})\"\n","c_df.iloc[1,2]=f\"({round(clan.iloc[2,2],3)},{round(clan.iloc[2,3],3)})\"\n","c_df.iloc[1,3]=f\"({round(clan.iloc[9,2],3)},{round(clan.iloc[9,3],3)})\"\n","c_df.iloc[1,4]=f\"({round(clan.iloc[10,2],3)},{round(clan.iloc[10,3],3)})\"\n","c_df.iloc[1,5]=f\"({round(clan.iloc[11,2],3)},{round(clan.iloc[11,3],3)})\"\n","#SES\n","c_df.iloc[4,0]=f\"({round(clan.iloc[3,2],3)},{round(clan.iloc[3,3],3)})\"\n","c_df.iloc[4,1]=f\"({round(clan.iloc[4,2],3)},{round(clan.iloc[4,3],3)})\"\n","c_df.iloc[4,2]=f\"({round(clan.iloc[5,2],3)},{round(clan.iloc[5,3],3)})\"\n","c_df.iloc[4,3]=f\"({round(clan.iloc[12,2],3)},{round(clan.iloc[12,3],3)})\"\n","c_df.iloc[4,4]=f\"({round(clan.iloc[13,2],3)},{round(clan.iloc[13,3],3)})\"\n","c_df.iloc[4,5]=f\"({round(clan.iloc[14,2],3)},{round(clan.iloc[14,3],3)})\"\n","#age\n","c_df.iloc[7,0]=f\"({round(clan.iloc[6,2],3)},{round(clan.iloc[6,3],3)})\"\n","c_df.iloc[7,1]=f\"({round(clan.iloc[7,2],3)},{round(clan.iloc[7,3],3)})\"\n","c_df.iloc[7,2]=f\"({round(clan.iloc[8,2],3)},{round(clan.iloc[8,3],3)})\"\n","c_df.iloc[7,3]=f\"({round(clan.iloc[15,2],3)},{round(clan.iloc[15,3],3)})\"\n","c_df.iloc[7,4]=f\"({round(clan.iloc[16,2],3)},{round(clan.iloc[16,3],3)})\"\n","c_df.iloc[7,5]=f\"({round(clan.iloc[17,2],3)},{round(clan.iloc[17,3],3)})\"\n","\n","#3rd row: SE\n","#income\n","c_df.iloc[2,0]=f\"-\"\n","c_df.iloc[2,1]=f\"-\"\n","c_df.iloc[2,2]=f\"[{round(clan.iloc[2,1],5)}]\"\n","c_df.iloc[2,3]=f\"-\"\n","c_df.iloc[2,4]=f\"-\"\n","c_df.iloc[2,5]=f\"[{round(clan.iloc[11,1],5)}]\"\n","#SES\n","c_df.iloc[5,0]=f\"-\"\n","c_df.iloc[5,1]=f\"-\"\n","c_df.iloc[5,2]=f\"[{round(clan.iloc[5,1],5)}]\"\n","c_df.iloc[5,3]=f\"-\"\n","c_df.iloc[5,4]=f\"-\"\n","c_df.iloc[5,5]=f\"[{round(clan.iloc[14,1],5)}]\"\n","#age\n","c_df.iloc[8,0]=f\"-\"\n","c_df.iloc[8,1]=f\"-\"\n","c_df.iloc[8,2]=f\"[{round(clan.iloc[8,1],5)}]\"\n","c_df.iloc[8,3]=f\"-\"\n","c_df.iloc[8,4]=f\"-\"\n","c_df.iloc[8,5]=f\"[{round(clan.iloc[17,1],5)}]\"\n","\n","\n","c_df.rename(columns={0:\"20% Most\", 1:\"20% Least\", 2:\"Difference\", 3:\"20% Most\", 4:\"20% Least\", 5:\"Difference\"},index={0: \"Income\",1:\"\",2:\"\",3:\"SES\",4:\"\",5:\"\",6:\"Age\",7:\"\",8:\"\"}, inplace=True)\n","#c_df.rename(columns={0:f\"{group_means[\"method\"][0]} 20% Most\", 1:f\"{group_means[\"method\"][0]} 20% Least\", 2:f\"{group_means[\"method\"][0]} Difference\", 3:f\"{group_means[\"method\"][9]} 20% Most\", 4:f\"{group_means[\"method\"][9]} 20% Least\", 5:f\"{group_means[\"method\"][9]} Difference\"},index={0: \"Income\",1:\"\",2:\"\",3:\"SES\",4:\"\",5:\"\",6:\"Age\",7:\"\",8:\"\"}, inplace=True)\n","print(c_df)\n","\n","c_df.to_csv(\"/content/drive/My Drive/ML/new2/Stage3_CLAN_satisfaction1.csv\", index=False, header=True)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["               20% Most        20% Least  ...        20% Least      Difference\n","Income            4.437            4.914  ...            4.826          -0.337\n","          (4.209,4.675)    (4.655,5.167)  ...    (4.606,5.067)  (-0.64,-0.014)\n","                      -                -  ...                -       [0.07831]\n","SES                0.95            0.962  ...            0.972          -0.009\n","          (0.938,0.963)     (0.95,0.973)  ...     (0.964,0.98)   (-0.02,0.003)\n","                      -                -  ...                -        [0.2337]\n","Age              14.203           12.491  ...           12.599           1.297\n","        (13.871,14.525)  (12.325,12.663)  ...  (12.378,12.816)    (0.88,1.695)\n","                      -                -  ...                -           [0.0]\n","\n","[9 rows x 6 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tzuLLQ2LfKZC"},"source":[""]}]}